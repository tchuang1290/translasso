---
title: "translasso"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{translasso}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(glmnet)
library(translasso)
```

In order to demonstrate how to best use the `translasso` package, we will run two simulation studies, one with Gaussian data and one with binomial data. These simulations will compare how well models fit with the `oracletranslasso()` function estimate $\beta$ coefficients compared to a generic Lasso model. Since this function models transfer learning, we need to generate two datasets for these simulations, the primary sample and the auxiliary sample. 

The first simulation will compare a Lasso model and the Oracle Trans Lasso algorithm with two similar samples. Since the Oracle Trans Lasso algorithm was originally conceived for use with genetic data, we generate our simulated data so that it has similarities with genetic data with a high number of features and less observations than features. We model a scenario where the primary sample has very few observations leading to the need for transfer learning for better coefficient estimates. Let's start with Gaussian data first.

```{r}
set.seed(2025) # set seed to ensure reproducibility

# initial values

p <- 1000 # number of features
n0 <- 100 # primary sample observations
n <- 500 # auxiliary sample observations
reps <- 10

# initialize lists to record differences between estimated
# and 'true' coefficients
delta1_list <- numeric(reps)
delta2_list <- numeric(reps)

for (r in 1:reps){
  # input data for primary (X0) and auxiliary (X1) samples
  X0 = matrix(rnorm(n0*p), nrow = n0, ncol = p)
  X1 = matrix(rnorm(n*p), nrow = n, ncol = p)
  
  # generate coefficients to ensure slight differences
  beta = runif(p, -2, 2) * sample(c(0,1), p, replace = TRUE, prob = c(0.9, 0.1))
  beta0 = beta + runif(p, -1, 1) * sample(c(0,1), p, replace = TRUE, prob = c(0.95, 0.05))
  
  # output for primary (y0) and auxiliary (y1) samples
  y0 = X0%*%beta0 + rnorm(n0)
  y1 = X1%*%beta + rnorm(n)
  
  # Fit oracle trans lasso model 
  oracle_beta = oracleTransLasso(X0,y0,X1,y1)
  
  # Fit Lasso model
  cvmodel = cv.glmnet(X0,y0, alpha = 1) 
  best_lambda = cvmodel$lambda.min
  best_model = glmnet(X0,y0, alpha = 1, lambda = best_lambda)
  non_oracle_beta = as.matrix(coef(best_model))
  
  # take sum of squares of difference between estimated versus 
  # true coefficients for each rep
  delta1_list[r] = sum((oracle_beta[-1] - beta0)^2)
  delta2_list[r] = sum((non_oracle_beta[-1] - beta0)^2)
}
```

Now we can look at the mean, median, and range of the two lists of differences between the estimated and true coefficients.

```{r}
gaussian_data <- data.frame(Mean = c(mean(delta1_list), mean(delta2_list)),
                             Median = c(median(delta1_list, median(delta2_list))),
                             Min = c(min(delta1_list, min(delta2_list))),
                             Max = c(max(delta1_list, max(delta2_list))))
gaussian_data
```

Now we can repeat this for binomial data. When using `oracletranslasso()` with binomial data, there are two important arguments that were not highlighted in the gaussian example: `family` and `nfolds`. 

The `family` argument essentially defines what kind of output data we are working with. The default for this argument is `"gaussian"` meaning that with gaussian data, we do not need to address it. However, for all other types of data, not changing this argument will return inaccurate results or result in other errors. For this simulation, we will define `family = "binomial"`. 

The other `nfolds`, whose default is 5, defines the number of folds for cross-validation when determining the best lambda for the generalized linear models. This argument is necessary to form stratified folds for binomial and multinomial data in order to ensure enough of each category is in each fold for the models to converge and have stability. While for most data this may not be a problem, for sparse data this is a necessary step to ensure quality results. 

The following simulation is essentially the same as the first with data generated in the same way except to account for generating a binomial output.

```{r}
set.seed(2025)

delta1_list <- numeric(reps)
delta2_list <- numeric(reps)

for (r in 1:reps){
  X0 = matrix(rnorm(n0*p), nrow = n0, ncol = p)
  X1 = matrix(rnorm(n*p), nrow = n, ncol = p)
  
  beta = runif(p, -2, 2) * sample(c(0,1),
                                  p,
                                  replace = TRUE,
                                  prob = c(0.9, 0.1))
  beta0 = beta + runif(p, -1, 1) * sample(c(0,1),
                                          p,
                                          replace = TRUE,
                                          prob = c(0.95, 0.05))
  
  z0 = X0%*%beta0
  z1 = X1%*%beta
  
  # convert normally distributed data into probabilities
  pr0 = 1/(1+exp(-z0))
  pr1 = 1/(1 + exp(-z1)) 
  
  y0 = rbinom(n0,1,pr0) # generate binomial results for primary sample
  y1 = rbinom(n,1,pr1) # generate binomial results for auxiliary sample
  
  oracle_beta = oracleTransLasso(X0,y0,X1,y1, family = "binomial")
  cvmodel = cv.glmnet(X0,y0,alpha = 1,family = "binomial")
  
  best_lambda = cvmodel$lambda.min
  best_model = glmnet(X0,y0, alpha = 1,
                      lambda = best_lambda,
                      family = "binomial")
  non_oracle_beta = as.matrix(coef(best_model))
  
  delta1_list[r] = sum((oracle_beta[-1] - beta0)^2)
  delta2_list[r] = sum((non_oracle_beta[-1] - beta0)^2)
}
```

Once again, we can compare the results of the `oracletranslasso()` function with the results of a standard Lasso model.

```{r}
binomial_data <- data.frame(Mean = c(mean(delta1_list), mean(delta2_list)),
                             Median = c(median(delta1_list, median(delta2_list))),
                             Min = c(min(delta1_list, min(delta2_list))),
                             Max = c(max(delta1_list, max(delta2_list))))
binomial_data
```

While the improvements are certainly not as large as with the Gaussian data, the estimated beta coefficients are still significantly closer to the true coefficients with the `oracletranslasso()` function than they are with the standard lasso model.


