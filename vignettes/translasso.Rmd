---
title: "translasso"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{translasso}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(translasso)
```

In order to demonstrate how to best use the `translasso` package, we will run three different simulation studies for each distribution accepted by the `oracletranslasso()` function. These simulations will compare how well models fit with the `oracletranslasso()` function estimate $\beta$ coefficients compared to a generic Lasso model. Since this function models transfer learning, we need to generate two datasets for these simulations, the primary sample and the auxiliary sample. 

The first simulation will compare a Lasso model and the Oracle Trans Lasso algorithm with two similar samples. Since the Oracle Trans Lasso algorithm was originally conceived for use with genetic data, we generate our simulated data so that it has similarities with genetic data with a high number of features and less observations than features. We model a scenario where the primary sample has very few observations leading to the need for transfer learning for better coefficient estimates. Let's start with gaussian data first.

```{r}
set.seed(2025) # set seed to ensure reproducibility

# initial values

p <- 1000 # number of features
n0 <- 100 # primary sample observations
n <- 500 # auxiliary sample observations
reps <- 10

# initialize lists to record differences between estimated
# and 'true' coefficients
delta1_list <- numeric(reps)
delta2_list <- numeric(reps)

for (r in 1:reps){
  # input data for primary (X0) and auxiliary (X1) samples
  X0 = matrix(rnorm(n0*p), nrow = n0, ncol = p)
  X1 = matrix(rnorm(n*p), nrow = n, ncol = p)
  
  # generate coefficients to ensure slight differences
  beta = runif(p, -2, 2) * sample(c(0,1), p, replace = TRUE, prob = c(0.9, 0.1))
  beta0 = beta + runif(p, -1, 1) * sample(c(0,1), p, replace = TRUE, prob = c(0.95, 0.05))
  
  # output for primary (y0) and auxiliary (y1) samples
  y0 = X0%*%beta0 + rnorm(n0)
  y1 = X1%*%beta + rnorm(n)
  
  # Fit oracle trans lasso model 
  oracle_beta = oracleTransLasso(X0,y0,X1,y1)
  
  # Fit Lasso model
  cvmodel = cv.glmnet(X0,y0, alpha = 1) 
  best_lambda = cvmodel$lambda.min
  best_model = glmnet(X0,y0, alpha = 1, lambda = best_lambda)
  non_oracle_beta = as.matrix(coef(best_model))
  
  # take sum of squares of difference between estimated versus 
  # true coefficients for each rep
  delta1_list[r] = sum((oracle_beta[-1] - beta0)^2)
  delta2_list[r] = sum((non_oracle_beta[-1] - beta0)^2)
}
```

Now we can look at the mean, median, and range of the two lists of differences between the estimated and true coefficients.

```{r}

```


